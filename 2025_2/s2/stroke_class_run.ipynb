{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "72976822",
   "metadata": {},
   "source": [
    "# Hands-On Workshop - Big Data in Healthcare 8400\n",
    "\n",
    "### Amit Levon - December 2025\n",
    "\n",
    "Welcome to this hands-on workshop on Big Data in Healthcare. In this workshop, we will perform Exploratory Data Analysis (EDA) to gain insights and intuition about a stroke records dataset.\n",
    "\n",
    "We will be covering the following topics:\n",
    "\n",
    "1. Introduction to the dataset and problem\n",
    "2. Data cleaning and preparation\n",
    "3. Exploratory Data Analysis (EDA)\n",
    "4. Feature engineering\n",
    "5. Model training and evaluation\n",
    "\n",
    "Let's get started!"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "a6466fb2",
   "metadata": {},
   "source": [
    "## Stroke Prediction Dataset\n",
    "\n",
    "The dataset we will be using in this workshop is the [Stroke Prediction Dataset](https://www.kaggle.com/fedesoriano/stroke-prediction-dataset) from **Kaggle**. It contains health records of over 5000 individuals, some of whom have suffered a stroke.\n",
    "\n",
    "According to the World Health Organization (WHO) stroke is the 2nd leading cause of death globally, responsible for approximately 11% of total deaths.\n",
    "This dataset can be used to predict whether a patient is likely to get stroke based on the input parameters like gender, age, various diseases, and smoking status. Each row in the data provides relevant information about the patient.\n",
    "\n",
    "We will be using this dataset to get initial insights about the data, and to get a *feel* for the work of a data scientist. We will understand how to use basic python scripting, packages and techniques to explore the data, and how to use this information to train a Machine Learning (ML) model in subsequent workshop.\n",
    "\n",
    "Hopefully, this workshop will give you a taste of what it's like to be a data scientist, and will demonstrate why it is beneficial to use python and Jupyter notebooks for data science."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4590da56",
   "metadata": {},
   "source": [
    "# Step 0: Imports and Reading Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b6bb04a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import NumPy: fundamental package for numerical computing with arrays and matrices\n",
    "import numpy as np\n",
    "\n",
    "# Import Pandas: data manipulation and analysis library with DataFrame structures\n",
    "import pandas as pd\n",
    "\n",
    "# Import Matplotlib: plotting library for creating static, interactive visualizations\n",
    "import matplotlib.pylab as plt\n",
    "\n",
    "# Import Seaborn: statistical data visualization library built on matplotlib\n",
    "import seaborn as sns\n",
    "\n",
    "# Set default matplotlib style to 'ggplot' for better-looking plots\n",
    "plt.style.use('ggplot')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a053a676",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download into this current session the data shared by Amit from his Google Drive.\n",
    "!wget --no-check-certificate 'https://docs.google.com/uc?export=download&id=19cNSK5MahWk4ErIf_Bs8aIJFkNUXUkBB' -O healthcare-dataset-stroke-data.csv\n",
    "\n",
    "# Read the CSV file containing stroke prediction data into a pandas DataFrame\n",
    "# The CSV file contains health records of 5,115 patients with 12 health/demographic features\n",
    "# and a target variable 'stroke' indicating whether the patient had a stroke (1) or not (0)\n",
    "df = pd.read_csv('healthcare-dataset-stroke-data.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b0d5b02",
   "metadata": {},
   "source": [
    "# Step 1: Data Understanding\n",
    "\n",
    "* Dataframe `shape`\n",
    "* `head` and `tail`\n",
    "*  `dtypes`\n",
    "*  `describe`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20d7265f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display shape of dataframe\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2703b8d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display first 10 rows of dataframe\n",
    "df.head(10)\n",
    "#df.tail(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "464c6f51",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display column names\n",
    "df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "501439e9",
   "metadata": {},
   "source": [
    "The current dataset contains the following features:\n",
    "\n",
    "* `id`: unique identifier\n",
    "* `gender`: \"Male\", \"Female\" or \"Other\"\n",
    "* `age`: age of the patient\n",
    "* `hypertension`: 0 if the patient doesn't have hypertension, 1 if the patient has hypertension\n",
    "* `heart_disease`: 0 if the patient doesn't have any heart diseases, 1 if the patient has a heart disease\n",
    "* `ever_married`: \"No\" or \"Yes\"\n",
    "* `work_type`: \"children\", \"Govt_jov\", \"Never_worked\", \"Private\" or \"Self-employed\"\n",
    "* `Residence_type`: \"Rural\" or \"Urban\"\n",
    "* `avg_glucose_level`: average glucose level in blood\n",
    "* `bmi`: body mass index\n",
    "* `smoking_status`: \"formerly smoked\", \"never smoked\", \"smokes\" or \"Unknown\"*\n",
    "* `stroke`: 1 if the patient had a stroke or 0 if not\n",
    "\n",
    "###### **Note**: \"Unknown\" in smoking_status means that the information is unavailable for this patient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d88807c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display information about types of data in dataframe\n",
    "df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "feb55bc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Describe basic statistics about dataframe\n",
    "df.describe()\n",
    "\n",
    "\n",
    "# AI Features:\n",
    "# 1) Please explain this code:\n",
    "# 2) Could you use more simple terms?\n",
    "# 3) Could you help me write code to analyze the other non-numeric columns?\n",
    "#cf.describe # Create an error!!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e18406c",
   "metadata": {},
   "source": [
    "# Step 2: Data Preparation\n",
    "\n",
    "* Dropping irrelevant columns and rows\n",
    "* Identifying duplicated columns\n",
    "* Renaming Columns\n",
    "* Feature Creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83d2cd54",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove columns that don't contribute to the analysis\n",
    "# 'irrelevant_column': contains non-informative data (all 'A' or 'C')\n",
    "# 'duplicate_column': duplicates avg_glucose_level values, adding no new information\n",
    "# axis=1 specifies we're dropping columns (not rows)\n",
    "# inplace=True modifies the dataframe directly without creating a new copy\n",
    "df.drop(['irrelevant_column', 'duplicate_column'], axis=1, inplace=True)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b1b4875",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rename columns to match standard naming conventions in the documentation\n",
    "# 'sex' → 'gender': more inclusive terminology for gender variable\n",
    "# 'patient_age' → 'age': shorter, cleaner column name\n",
    "# inplace=True modifies the original dataframe\n",
    "df.rename(columns={'sex': 'gender', 'patient_age': 'age'}, inplace=True)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df592bc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identifying duplicate rows\n",
    "df.duplicated()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f587fc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display all rows that are duplicates\n",
    "# This shows us which rows have identical values\n",
    "# Useful for understanding what duplicate data we have\n",
    "df[df.duplicated()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a7a9877",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove all duplicate rows from the dataframe\n",
    "# Keeps the first occurrence of each duplicate row by default\n",
    "# Updates the dataframe and returns the new shape to confirm duplicates were removed\n",
    "df = df.drop_duplicates()\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96c5073f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking the number of missing values\n",
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac41445b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the percentage of missing BMI values\n",
    "# Divides missing count by total rows, multiplies by 100 for percentage\n",
    "# Helps decide whether to drop rows or impute missing values\n",
    "# Rule of thumb: if > 5% missing, consider imputation; if > 20%, may want to drop\n",
    "df['bmi'].isnull().sum() / len(df) * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b1aacc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Impute missing BMI values with the column mean\n",
    "# df['bmi'].mean() calculates the average BMI of all non-missing values\n",
    "# fillna() replaces all NaN values with this mean\n",
    "# This is a simple imputation strategy; advanced methods (KNN, MICE) could be used\n",
    "# inplace=True modifies the original column\n",
    "df['bmi'].fillna(df['bmi'].mean(), inplace=True)\n",
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d8921dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# CREATE NEW FEATURE: Health Risk Score\n",
    "# ==============================================================================\n",
    "# This engineered feature combines multiple health indicators into a single score\n",
    "# Each component is normalized to a 0-1 scale for fair weighting:\n",
    "\n",
    "# Calculate raw health risk score by summing normalized health indicators:\n",
    "# - age/100: normalized age (assuming max age ~100)\n",
    "# - hypertension: binary flag (0 or 1)\n",
    "# - heart_disease: binary flag (0 or 1)\n",
    "# - avg_glucose_level/200: normalized glucose (assuming normal range 0-200)\n",
    "# - bmi/50: normalized BMI (assuming max healthy BMI ~50)\n",
    "\n",
    "df['health_risk_score'] = (\n",
    "    (df['age'] / 100) +\n",
    "    df['hypertension'] +\n",
    "    df['heart_disease'] +\n",
    "    (df['avg_glucose_level'] / 200) +\n",
    "    (df['bmi']) / 50)\n",
    "\n",
    "# Min-Max normalize the health_risk_score to scale between 0 and 1\n",
    "# This ensures the final score is comparable and easier to interpret\n",
    "# Formula: (x - min) / (max - min) scales any distribution to [0, 1] range\n",
    "df['health_risk_score'] = (df['health_risk_score'] - df['health_risk_score'].min()) / (df['health_risk_score'].max() - df['health_risk_score'].min())\n",
    "\n",
    "# Display first few rows to verify the new feature was created\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f535b00",
   "metadata": {},
   "source": [
    "# Step 3: Feature Understanding - Univariate Analysis\n",
    "\n",
    "* Plotting Feature Distributions\n",
    "  * Histogram\n",
    "  * KDE\n",
    "  * Boxplot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d01c54f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count the frequency of each smoking status category\n",
    "# value_counts() returns counts sorted in descending order\n",
    "# Shows the distribution of smoking statuses in our dataset\n",
    "df['smoking_status'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc29b239",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a bar plot of smoking status distribution\n",
    "# .value_counts() gets frequency of each category\n",
    "# .head(10) limits to top 10 categories (though smoking_status has only 4)\n",
    "# kind='bar' creates a bar chart\n",
    "# figsize=(7, 5) sets plot dimensions\n",
    "# color='green' sets bar color\n",
    "# fontsize=10\n",
    "ax = df['smoking_status'].value_counts() \\\n",
    "    .head(10) \\\n",
    "    .plot(kind='bar', title='Smoking Status', figsize=(7, 5), color='green', fontsize=10)\n",
    "ax.set_xlabel('Smoking Status')\n",
    "ax.set_ylabel('Count')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c21117ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# EXPLORATORY DATA ANALYSIS: Dual Histogram with Twin Axis\n",
    "# ==============================================================================\n",
    "# This visualization shows the distribution of two continuous variables simultaneously\n",
    "\n",
    "# Create a figure with a single axis\n",
    "# figsize=(15, 10) sets the plot size\n",
    "fig, ax1 = plt.subplots(figsize=(15, 10))\n",
    "\n",
    "# Plot histogram of 'age' on the first y-axis\n",
    "# bins=20 divides the age range into 20 bins\n",
    "# alpha=0.5 sets transparency to 50% so overlapping bars are visible\n",
    "# color='blue' colors age bars blue\n",
    "# label='Age' provides legend text\n",
    "ax1.hist(df['age'], bins=20, alpha=0.5, color='blue', label='Age')\n",
    "\n",
    "# Set labels and font sizes for the first axis\n",
    "ax1.set_xlabel('Age', fontsize=20)\n",
    "ax1.set_ylabel('Count of People with Age', fontsize=20)\n",
    "ax1.tick_params(axis='both', which='major', labelsize=20)\n",
    "\n",
    "# Create a second y-axis that shares the same x-axis\n",
    "# This allows plotting two different scales on the same figure\n",
    "ax2 = ax1.twinx()\n",
    "\n",
    "# Plot histogram of 'avg_glucose_level' on the second y-axis\n",
    "# The data has different scale than age, so a separate axis is needed\n",
    "ax2.hist(df['avg_glucose_level'], bins=20, alpha=0.5, color='green', label='Avg Glucose Level')\n",
    "\n",
    "# Set labels for the second y-axis\n",
    "ax2.set_ylabel('Count of People with Avg Glucose Level', fontsize=20)\n",
    "ax2.tick_params(axis='y', which='major', labelsize=20)\n",
    "\n",
    "# Add legends from both axes for complete information\n",
    "ax1.legend(loc='upper left', fontsize=15)\n",
    "ax2.legend(loc='upper right', fontsize=15)\n",
    "\n",
    "# Showing the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c62a90f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# VISUALIZATION: Health Risk Score Distribution using KDE\n",
    "# =============================================================================\n",
    "\n",
    "# Create a new figure with specified size\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "# Kernel Density Estimation (KDE) plot:\n",
    "# KDE is a statistical technique that estimates the probability density function\n",
    "# of a continuous variable by smoothing the distribution\n",
    "# \n",
    "# Key parameters:\n",
    "#   - df['health_risk_score']: the data to visualize (values range from 0 to 1)\n",
    "#   - shade=True: fills the area under the curve with color (red in this case)\n",
    "#     This makes the distribution more visually prominent\n",
    "#   - color=\"r\": uses red color, which stands out and emphasizes the distribution\n",
    "#\n",
    "# Why use KDE instead of histogram?\n",
    "#   - KDE shows a smooth, continuous curve rather than discrete bars\n",
    "#   - Better for identifying overall distribution shape\n",
    "#   - Easier to spot multiple peaks (modes) if they exist\n",
    "#   - More professional appearance for presentations\n",
    "#\n",
    "# What to look for:\n",
    "#   - Shape: Is the distribution normal (bell-shaped), skewed, or has multiple peaks?\n",
    "#   - Location: Where are most health risk scores concentrated?\n",
    "#   - Spread: How wide is the distribution? (Narrow = consistent risk, Wide = variable risk)\n",
    "#   - Outliers: Are there extreme values (very low or very high risk scores)?\n",
    "sns.kdeplot(df['health_risk_score'], shade=True, color=\"r\")\n",
    "\n",
    "# Set the title of the plot\n",
    "plt.title('KDE of Health Risk Score')\n",
    "\n",
    "# Label the x-axis\n",
    "# The x-axis shows the health risk score values (0 = lowest risk, 1 = highest risk)\n",
    "plt.xlabel('Health Risk Score')\n",
    "\n",
    "# Label the y-axis\n",
    "# The y-axis shows density (probability), not count\n",
    "# Higher density = more patients with that risk score value\n",
    "plt.ylabel('Density')\n",
    "\n",
    "# Display the plot in the notebook\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66107df5",
   "metadata": {},
   "source": [
    "# Step 4: Feature Relationship - Bivariate Analysis\n",
    "\n",
    "* Scatterplot\n",
    "* Heatmap Correlation\n",
    "* Pairplot\n",
    "* Groupby comparisons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ed1e796",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# VISUALIZATION: Age vs Health Risk Score with Stroke Outcome Coloring\n",
    "# =============================================================================\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "\n",
    "# Scatter plot: plots individual data points showing relationship between two variables\n",
    "# This visualization helps identify patterns, clusters, and potential correlations\n",
    "#\n",
    "# Key parameters explained:\n",
    "#   - x=df['age']: x-axis represents patient age (in years, typically 0-82 in this dataset)\n",
    "#   - y=df['health_risk_score']: y-axis represents the composite health risk score we created\n",
    "#     (values range from 0 to 1, where 0 = lowest risk, 1 = highest risk)\n",
    "#\n",
    "#   - hue=df['stroke']: This parameter colors points based on stroke status\n",
    "#     hue = \"shade\" or \"color\" in visualization terminology\n",
    "#     Each unique value in df['stroke'] gets a different color\n",
    "#     Stroke=0: no stroke (blue), Stroke=1: had stroke (red)\n",
    "#     This allows us to visually compare risk profiles between stroke/no-stroke patients\n",
    "#\n",
    "#   - palette=['blue', 'red']: defines the color scheme\n",
    "#     Blue = safer/no event, Red = danger/event occurred\n",
    "#\n",
    "#   - alpha=0.5: transparency level (0=invisible, 1=opaque)\n",
    "#     alpha=0.5 means 50% transparency\n",
    "#     When points overlap (because many patients have similar age/risk scores),\n",
    "#     we can see through overlapping points to the points beneath them\n",
    "#     Darker areas indicate high concentration of patients (overlapping points)\n",
    "#     This reveals hidden patterns in dense areas of data\n",
    "#\n",
    "# What patterns to look for:\n",
    "#   - Do blue points cluster in lower age/lower risk areas? (expected for healthy)\n",
    "#   - Do red points cluster in higher age/higher risk areas? (expected for stroke risk)\n",
    "#   - Are there any red points in low-risk areas? (surprising cases)\n",
    "#   - Is there a clear separation between blue and red? (indicates age/risk are good predictors)\n",
    "#   - Do we see a trend where risk increases with age?\n",
    "sns.scatterplot(\n",
    "    x=df['age'], \n",
    "    y=df['health_risk_score'],\n",
    "    hue=df['stroke'],\n",
    "    palette=['blue', 'red'],\n",
    "    alpha=0.5)\n",
    "\n",
    "# Set the plot title with specified font size\n",
    "plt.title('Scatter Plot of Age and Health Risk Score', fontsize=20)\n",
    "\n",
    "# Label the x-axis with specified font size\n",
    "# X-axis = Age: shows patient age in years\n",
    "plt.xlabel('Age', fontsize=10)\n",
    "\n",
    "# Label the y-axis with specified font size\n",
    "# Y-axis = Health Risk Score: our engineered feature combining multiple health factors\n",
    "#   Range: 0 to 1 (0 = lowest risk, 1 = highest risk)\n",
    "#   Higher scores indicate worse overall health profile\n",
    "plt.ylabel('Health Risk Score', fontsize=10)\n",
    "\n",
    "# Display the plot in the notebook\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d40a36cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# VISUALIZATION: Age vs Health Risk Score with Linear Regression Trend Line\n",
    "# =============================================================================\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "\n",
    "# Regression plot: combines scatter plot with linear regression analysis\n",
    "# This visualization shows both individual data points AND the overall trend\n",
    "# \n",
    "# Linear regression finds the \"best-fit\" straight line through the data\n",
    "# The line equation: health_risk_score = intercept + slope * age\n",
    "# \n",
    "# Key parameters explained:\n",
    "#   - x=df['age']: x-axis variable (patient age in years)\n",
    "#   - y=df['health_risk_score']: y-axis variable (our engineered health risk score)\n",
    "#     The regression line predicts y-values based on x-values\n",
    "#     Equation: predicted_score = a + b*age\n",
    "#     If b (slope) is positive: older age → higher risk score (expect this)\n",
    "#     If b (slope) is negative: older age → lower risk score (unexpected)\n",
    "#     If b (slope) ≈ 0: age has little effect on risk score (weak relationship)\n",
    "#\n",
    "#   - color='green': base color for the visualization\n",
    "#     Used as default color when scatter_kws and line_kws don't override\n",
    "#\n",
    "#   - scatter_kws={'alpha': 0.5}: keyword arguments for scatter points\n",
    "#     'kws' = keyword arguments (configuration options)\n",
    "#     scatter_kws = options specifically for the scatter plot portion\n",
    "#     alpha=0.5 makes points 50% transparent\n",
    "#     Shows clustering: darker areas = more overlapping points = more patients there\n",
    "#     This reveals which age/risk combinations are most common in our dataset\n",
    "#\n",
    "#   - line_kws={'color': 'red'}: keyword arguments for the regression line\n",
    "#     line_kws = options specifically for the regression line\n",
    "#     color='red' makes the line stand out prominently against green scatter points\n",
    "#     Red line = easy to see the overall trend direction\n",
    "#     Visually distinguishes the trend from individual data points\n",
    "sns.regplot(\n",
    "    x=df['age'], \n",
    "    y=df['health_risk_score'], \n",
    "    color='green',\n",
    "    scatter_kws={'alpha': 0.5}, \n",
    "    line_kws={'color': 'red'})\n",
    "\n",
    "plt.title('Scatter Plot of Age and Health Risk Score', fontsize=20)\n",
    "\n",
    "plt.xlabel('Age', fontsize=10)\n",
    "\n",
    "plt.ylabel('Health Risk Score', fontsize=10)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88dcf7ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a pairplot matrix showing relationships between multiple variables\n",
    "# A pairplot shows scatter plots for all pairs of variables\n",
    "# Diagonal shows distributions of each variable\n",
    "\n",
    "# Use seaborn's pairplot to create a matrix of relationships\n",
    "# df: the dataframe to plot\n",
    "# vars=[...]: specific variables to include (numeric features of interest)\n",
    "# hue='stroke': color points by stroke status for pattern detection\n",
    "# palette=['blue', 'red']: blue=no stroke, red=stroke\n",
    "sns.pairplot(\n",
    "    df,\n",
    "    vars=['age', 'avg_glucose_level', 'bmi', 'hypertension', 'heart_disease'],\n",
    "    hue='stroke',\n",
    "    palette=['blue', 'red']\n",
    ")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4f11821",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# EXPLORATORY DATA ANALYSIS: Multi-panel Analysis of Key Features\n",
    "# ==============================================================================\n",
    "# Create a 3x3 grid of plots analyzing three key health features\n",
    "\n",
    "# Create a 3x3 subplot grid (3 rows, 3 columns)\n",
    "# Each row analyzes one health feature (age, glucose, BMI) in three ways\n",
    "fig, axes = plt.subplots(nrows=3, ncols=3, figsize=(16, 12))\n",
    "\n",
    "# ==============================================================================\n",
    "# ROW 1: Age Analysis\n",
    "# ==============================================================================\n",
    "# Box plot of age (shows quartiles, median, outliers)\n",
    "sns.boxplot(x=df['age'], ax=axes[0, 0]).set_title(\"**BoxPlot For Age Col**\")\n",
    "# Histogram with KDE curve (shows distribution shape)\n",
    "sns.histplot(data=df, x='age', kde=True, ax=axes[0, 1]).set_title(\"**Distribution Of Age**\")\n",
    "# Line plot of age vs stroke (shows trend with target variable)\n",
    "sns.lineplot(data=df, x='age', y=\"stroke\", ax=axes[0, 2]).set_title('**Lineplot For Age with Stroke**')\n",
    "\n",
    "# ==============================================================================\n",
    "# ROW 2: Average Glucose Level Analysis\n",
    "# ==============================================================================\n",
    "# Box plot of glucose levels\n",
    "sns.boxplot(x=df['avg_glucose_level'], ax=axes[1, 0]).set_title(\"BoxPlot For Glucose\")\n",
    "# Histogram with KDE of glucose levels\n",
    "sns.histplot(data=df, x='avg_glucose_level', kde=True, ax=axes[1, 1]).set_title(\"Distribution Of Glucose\")\n",
    "# Line plot showing glucose relationship with stroke\n",
    "sns.lineplot(data=df, x='avg_glucose_level', y=\"stroke\", ax=axes[1, 2]).set_title('**Lineplot For Glucose Level With Stroke')\n",
    "\n",
    "# ==============================================================================\n",
    "# ROW 3: BMI Analysis\n",
    "# ==============================================================================\n",
    "# Box plot of BMI values\n",
    "sns.boxplot(x=df['bmi'], ax=axes[2, 0]).set_title(\"BoxPlot For Bmi Col\")\n",
    "# Histogram with KDE of BMI distribution\n",
    "sns.histplot(data=df, x='bmi', kde=True, ax=axes[2, 1]).set_title(\"Distribution Of Bmi\")\n",
    "# Line plot of BMI vs stroke outcome\n",
    "sns.lineplot(data=df, x='bmi', y=\"stroke\", ax=axes[2, 2]).set_title('Lineplot For Bmi With Stroke')\n",
    "\n",
    "# Automatically adjust spacing to prevent label overlap\n",
    "plt.tight_layout()\n",
    "# Display the complete grid of plots\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "454b06e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a correlation heatmap of numeric features\n",
    "# Shows how strongly pairs of variables are related\n",
    "\n",
    "# Create a new figure\n",
    "plt.figure(figsize=(10, 5))\n",
    "\n",
    "# Calculate correlation matrix for selected numeric features\n",
    "# .corr() computes Pearson correlation coefficients between all variables\n",
    "# Values range from -1 (perfect negative) to +1 (perfect positive) correlation\n",
    "# Create heatmap visualization\n",
    "# annot=True displays correlation values in each cell\n",
    "# cmap='coolwarm': uses blue-red colormap (blue=negative, red=positive correlations)\n",
    "sns.heatmap(\n",
    "    df[['age', 'avg_glucose_level', 'bmi', 'hypertension', 'heart_disease']].corr(), \n",
    "    annot=True,\n",
    "    cmap='coolwarm')\n",
    "\n",
    "# We can add 'health_risk_score'.\n",
    "\n",
    "# Add title\n",
    "plt.title('Correlation Matrix')\n",
    "\n",
    "# Display the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d91c4277",
   "metadata": {},
   "source": [
    "# Step 5: Stroke Prediction Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89b63501",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# MODEL PREPARATION: ENCODE CATEGORICAL VARIABLES\n",
    "# ==============================================================================\n",
    "# Convert categorical (text) features to numeric codes for machine learning models\n",
    "# Models require numeric input, so we map categories to integers\n",
    "df_ml = df.copy()  # For model training (with encoding)\n",
    "\n",
    "# Encode 'gender' column\n",
    "# Male → 0, Female → 1, Other → -1\n",
    "# astype(np.uint8) converts to unsigned integer (uses less memory)\n",
    "df_ml['gender'] = df_ml['gender'].replace({'Male':0,'Female':1,'Other':-1}).astype(np.uint8)\n",
    "\n",
    "# Encode 'Residence_type' column\n",
    "# Rural → 0, Urban → 1\n",
    "df_ml['Residence_type'] = df_ml['Residence_type'].replace({'Rural':0,'Urban':1}).astype(np.uint8)\n",
    "\n",
    "# Encode 'work_type' column (5 categories need 5 different codes)\n",
    "# Private → 0, Self-employed → 1, Govt_job → 2\n",
    "# children → -1, Never_worked → -2 (special codes for age-specific groups)\n",
    "df_ml['work_type'] = df_ml['work_type'].replace({'Private':0,'Self-employed':1,'Govt_job':2,'children':-1,'Never_worked':-2}).astype(np.uint8)\n",
    "\n",
    "# Display updated dataframe with encoded variables\n",
    "df_ml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34c3e145",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# MACHINE LEARNING: PREPARE FEATURES (X) AND TARGET (y)\n",
    "# ==============================================================================\n",
    "\n",
    "# Define feature matrix X: select columns to use for model training\n",
    "# These are the input variables (predictors) the model will learn from\n",
    "X  = df_ml[['gender','age','hypertension','heart_disease','work_type','avg_glucose_level','bmi']]\n",
    "\n",
    "# Define target variable y: what we want to predict\n",
    "# stroke: 0=No stroke, 1=Stroke occurred\n",
    "y = df_ml['stroke']\n",
    "\n",
    "# Import train_test_split function for dividing data into training and testing sets\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Split data into training (70%) and testing (30%) sets\n",
    "# This allows us to train the model on one subset and evaluate on unseen data\n",
    "# train_size=0.3 means use 70% for training (30% for testing)\n",
    "# random_state=42 ensures reproducible splits for consistency across runs\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.7, random_state=42)\n",
    "\n",
    "# Display first 2 rows of test set to verify the split worked correctly\n",
    "X_test.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfe8a6be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# MACHINE LEARNING: CREATE ML PIPELINES WITH SCALING AND CLASSIFIERS\n",
    "# ==============================================================================\n",
    "# Pipelines combine preprocessing (scaling) with model training in a single workflow\n",
    "# This ensures consistent processing of training and test data\n",
    "\n",
    "# Import required libraries\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# STANDARDSCALER\n",
    "# ======================================================================\n",
    "# StandardScaler transforms features to have mean 0 and standard deviation 1,\n",
    "# which normalizes all features to the same scale so that features with large \n",
    "# values (like age or glucose level) don't dominate the machine learning model \n",
    "# over features with smaller values.\n",
    "\n",
    "# ==============================================================================\n",
    "# PIPELINE 1: Random Forest Classifier with StandardScaler\n",
    "# ==============================================================================\n",
    "# RandomForestClassifier: ensemble of decision trees, robust to outliers\n",
    "# Random Forest Classifier builds many decision trees (each asking yes/no \n",
    "# questions about features like \"Is age > 60?\" or \"Does patient have hypertension?\"), \n",
    "# then combines their predictions by majority vote to make a final stroke prediction.\n",
    "\n",
    "# Step 1: Normalize features\n",
    "# Step 2: Train RF model\n",
    "rf_pipeline = Pipeline(steps = [('scale',StandardScaler()),('RF',RandomForestClassifier(random_state=42))])\n",
    "\n",
    "# ==============================================================================\n",
    "# PIPELINE 2: Support Vector Machine with StandardScaler\n",
    "# ==============================================================================\n",
    "# SVC: finds optimal hyperplane to separate classes, good for high-dimensional data\n",
    "# Support Vector Machine finds the optimal boundary (hyperplane) that \n",
    "# maximally separates stroke patients from non-stroke patients while \n",
    "# leaving the largest margin of safety between the two groups.\n",
    "\n",
    "# Step 1: Normalize features\n",
    "# Step 2: Train SVM model\n",
    "svm_pipeline = Pipeline(steps = [('scale',StandardScaler()),('SVM',SVC(random_state=42))])\n",
    "\n",
    "# ==============================================================================\n",
    "# PIPELINE 3: Logistic Regression with StandardScaler\n",
    "# ==============================================================================\n",
    "# LogisticRegression: linear model for binary classification, interpretable\n",
    "# Logistic Regression predicts the probability of stroke by finding the \n",
    "# best-fitting line/curve that separates patients with stroke from those \n",
    "# without stroke, using a mathematical formula that outputs values between 0 and 1.\n",
    "\n",
    "# Step 1: Normalize features\n",
    "# Step 2: Train LR model\n",
    "logreg_pipeline = Pipeline(steps = [('scale',StandardScaler()),('LR',LogisticRegression(random_state=42))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "908e00fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the class distribution of the target variable\n",
    "# Count how many samples are in each class (stroke vs no stroke)\n",
    "# This reveals if the data is balanced or imbalanced\n",
    "df_ml['stroke'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edbf9de3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# HANDLING CLASS IMBALANCE: SMOTE (Synthetic Minority Over-sampling Technique)\n",
    "# ==============================================================================\n",
    "# Our data has far fewer stroke cases (positive class) than non-stroke (negative class)\n",
    "# This imbalance can bias the model toward the majority class\n",
    "# SMOTE synthetically generates new samples of the minority class\n",
    "\n",
    "# Import SMOTE from imbalanced-learn library\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "# Create SMOTE resampler object\n",
    "# This will generate synthetic samples for the minority class\n",
    "oversample = SMOTE()\n",
    "\n",
    "# Apply SMOTE to training data\n",
    "# fit_resample(): generates synthetic minority samples to balance the data\n",
    "# X_train_resh, y_train_resh: resampled training features and labels\n",
    "# Now X_train_resh has balanced number of stroke and non-stroke cases\n",
    "X_train_resh, y_train_resh = oversample.fit_resample(X_train, y_train.ravel())\n",
    "\n",
    "# UNDERSAMPLING\n",
    "# ======================================================================\n",
    "# Undersampling randomly removes majority class samples (non-stroke patients) \n",
    "# to match the number of minority class samples (stroke patients), creating \n",
    "# a balanced dataset but discarding potentially useful data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "145906b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# MODEL EVALUATION: CROSS-VALIDATION USING F1 SCORE\n",
    "# ==============================================================================\n",
    "# Cross-validation: train and evaluate model on multiple data splits\n",
    "# F1 score: harmonic mean of precision and recall (good for imbalanced data)\n",
    "# More robust than accuracy for imbalanced classification problems\n",
    "\n",
    "# Import cross-validation function\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "# Print header for results\n",
    "print('Mean f1 scores:')\n",
    "\n",
    "# Evaluate Random Forest Classifier\n",
    "# cv=10: 10-fold cross-validation (split data into 10 folds)\n",
    "# scoring='f1': use F1 score as evaluation metric\n",
    "# .mean(): calculate average F1 score across all 10 folds\n",
    "print('Random Forest mean :',cross_val_score(rf_pipeline,X_train_resh,y_train_resh,cv=10,scoring='f1').mean())\n",
    "\n",
    "# Evaluate Support Vector Machine\n",
    "print('SVM mean :',cross_val_score(svm_pipeline,X_train_resh,y_train_resh,cv=10,scoring='f1').mean())\n",
    "\n",
    "# Evaluate Logistic Regression\n",
    "print('Logistic Regression mean :',cross_val_score(logreg_pipeline,X_train_resh,y_train_resh,cv=10,scoring='f1').mean())\n",
    "\n",
    "# F1 SCORE\n",
    "# ======================================================================\n",
    "# F1 Score is the harmonic mean of precision and recall, measuring how well \n",
    "# the model balances correctly identifying stroke patients (recall) with \n",
    "# avoiding false alarms (precision), making it ideal for imbalanced datasets \n",
    "# where both missed strokes and unnecessary alerts are costly.\n",
    "# Precision = Of patients we predict will have stroke, how many actually do?\n",
    "# Recall = Of patients who actually had stroke, how many did we catch?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8be02cc",
   "metadata": {},
   "source": [
    "# Step 6: Ask a Question about the data\n",
    "* Try to answer a question you have about the data using a plot or statistic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "512f5dc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# Identify Which Groups Are Most Susceptible to Stroke\n",
    "# ==============================================================================\n",
    "# Analyze stroke risk by demographic groups (residence type + work type)\n",
    "\n",
    "# Filter for stroke cases and group by residence and work type\n",
    "# df[df['stroke'] == 1]: select only rows where a stroke occurred\n",
    "# .groupby(['Residence_type', 'work_type']): group by these two variables\n",
    "# .size(): count cases in each group\n",
    "# .reset_index(name='count'): convert to dataframe with 'count' column\n",
    "stroke_susceptibility = df[df['stroke'] == 1].groupby(['Residence_type', 'work_type']).size().reset_index(name='count')\n",
    "\n",
    "# Sort by stroke count in descending order\n",
    "# Shows which groups have the highest number of stroke cases\n",
    "stroke_susceptibility = stroke_susceptibility.sort_values('count', ascending=False)\n",
    "\n",
    "# Create a combined label for better visualization\n",
    "# Combines 'Residence_type' and 'work_type' into a single string\n",
    "stroke_susceptibility['Residence_Work'] = stroke_susceptibility['Residence_type'] + \", \" + stroke_susceptibility['work_type']\n",
    "\n",
    "# Create horizontal bar plot\n",
    "# kind='barh': horizontal bar chart\n",
    "# x='Residence_Work': labels on y-axis (grouped categories)\n",
    "# y='count': bar lengths (number of strokes)\n",
    "# figsize=(20, 7): wide plot for readability\n",
    "# title: descriptive title\n",
    "ax = stroke_susceptibility.plot(\n",
    "    kind='barh',\n",
    "    x='Residence_Work',\n",
    "    y='count',\n",
    "    figsize=(20, 7),\n",
    "    title='Number of Strokes by Residence and Work Type'\n",
    ")\n",
    "\n",
    "ax.set_xlabel('Number of Strokes')\n",
    "ax.set_ylabel('Residence Type, Work Type')\n",
    "\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Tags",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
